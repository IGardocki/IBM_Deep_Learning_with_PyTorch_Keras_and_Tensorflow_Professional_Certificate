{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Custom Training Loops in Keras**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn to implement a basic custom training loop in Keras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "- Set up the environment \n",
    "\n",
    "- Define the neural network model \n",
    "\n",
    "- Define the Loss Function and Optimizer \n",
    "\n",
    "- Implement the custom training loop \n",
    "\n",
    "- Enhance the custom training loop by adding an accuracy metric to monitor model performance \n",
    "\n",
    "- Implement a custom callback to log additional metrics and information during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic custom training loop: \n",
    "\n",
    "#### 1. Set Up the Environment:\n",
    "\n",
    "- Import necessary libraries. \n",
    "\n",
    "- Load and preprocess the MNIST dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.70.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.5/615.5 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.70.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (403 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 h5py-3.13.0 keras-3.9.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 opt-einsum-3.4.0 optree-0.14.1 protobuf-5.29.3 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0 werkzeug-3.1.3 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 05:16:04.438082: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-09 05:16:04.439848: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-09 05:16:04.446488: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-09 05:16:04.458897: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741497364.479797     299 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741497364.485390     299 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-09 05:16:04.515257: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 05:16:10.001546: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "# Suppress all Python warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set TensorFlow log level to suppress warnings and info messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the model: \n",
    "\n",
    "Create a simple neural network model with a Flatten layer followed by two Dense layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define Loss Function and Optimizer: \n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function. \n",
    "- Use the Adam optimizer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function and Optimizer\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement the Custom Training Loop: \n",
    "\n",
    "- Iterate over the dataset for a specified number of epochs. \n",
    "- Compute the loss and apply gradients to update the model's weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.5056424140930176\n",
      "Epoch 1 Step 200: Loss = 0.38649412989616394\n",
      "Epoch 1 Step 400: Loss = 0.17723262310028076\n",
      "Epoch 1 Step 600: Loss = 0.18185792863368988\n",
      "Epoch 1 Step 800: Loss = 0.16443389654159546\n",
      "Epoch 1 Step 1000: Loss = 0.40792179107666016\n",
      "Epoch 1 Step 1200: Loss = 0.19710171222686768\n",
      "Epoch 1 Step 1400: Loss = 0.23565560579299927\n",
      "Epoch 1 Step 1600: Loss = 0.20582465827465057\n",
      "Epoch 1 Step 1800: Loss = 0.19570569694042206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 05:17:03.972306: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.06841857731342316\n",
      "Epoch 2 Step 200: Loss = 0.19628199934959412\n",
      "Epoch 2 Step 400: Loss = 0.08695618063211441\n",
      "Epoch 2 Step 600: Loss = 0.04483775794506073\n",
      "Epoch 2 Step 800: Loss = 0.08464257419109344\n",
      "Epoch 2 Step 1000: Loss = 0.24966688454151154\n",
      "Epoch 2 Step 1200: Loss = 0.09584302455186844\n",
      "Epoch 2 Step 1400: Loss = 0.16298791766166687\n",
      "Epoch 2 Step 1600: Loss = 0.17934547364711761\n",
      "Epoch 2 Step 1800: Loss = 0.10747847706079483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 05:17:52.717864: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement the Custom Training Loop\n",
    "\n",
    "epochs = 2\n",
    "# train_dataset = train_dataset.repeat(epochs)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)  # Forward pass\n",
    "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Logging the loss every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Adding Accuracy Metric:\n",
    "\n",
    "Enhance the custom training loop by adding an accuracy metric to monitor model performance. \n",
    "\n",
    "#### 1. Set Up the Environment: \n",
    "\n",
    "Follow the setup from Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the Model: \n",
    "Use the same model as in Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define the loss function, optimizer, and metric: \n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function and Adam optimizer. \n",
    "\n",
    "- Add Sparse Categorical Accuracy as a metric. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement the custom training loop with accuracy: \n",
    "\n",
    "Track the accuracy during training and print it at regular intervals. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.4367899894714355 Accuracy = 0.0625\n",
      "Epoch 1 Step 200: Loss = 0.41094523668289185 Accuracy = 0.8356654047966003\n",
      "Epoch 1 Step 400: Loss = 0.17091906070709229 Accuracy = 0.8682200908660889\n",
      "Epoch 1 Step 600: Loss = 0.14841966331005096 Accuracy = 0.8828514814376831\n",
      "Epoch 1 Step 800: Loss = 0.18326741456985474 Accuracy = 0.8950140476226807\n",
      "Epoch 1 Step 1000: Loss = 0.41128939390182495 Accuracy = 0.9025037288665771\n",
      "Epoch 1 Step 1200: Loss = 0.16701440513134003 Accuracy = 0.9087739586830139\n",
      "Epoch 1 Step 1400: Loss = 0.26267480850219727 Accuracy = 0.9134992957115173\n",
      "Epoch 1 Step 1600: Loss = 0.23584482073783875 Accuracy = 0.9168097972869873\n",
      "Epoch 1 Step 1800: Loss = 0.1338028460741043 Accuracy = 0.9207211136817932\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.08427025377750397 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.19367150962352753 Accuracy = 0.961442768573761\n",
      "Epoch 2 Step 400: Loss = 0.0871322900056839 Accuracy = 0.9579956531524658\n",
      "Epoch 2 Step 600: Loss = 0.05051877722144127 Accuracy = 0.9593386054039001\n",
      "Epoch 2 Step 800: Loss = 0.09497415274381638 Accuracy = 0.9604010581970215\n",
      "Epoch 2 Step 1000: Loss = 0.2821457087993622 Accuracy = 0.9612262845039368\n",
      "Epoch 2 Step 1200: Loss = 0.09494737535715103 Accuracy = 0.9620628356933594\n",
      "Epoch 2 Step 1400: Loss = 0.15995298326015472 Accuracy = 0.9628167152404785\n",
      "Epoch 2 Step 1600: Loss = 0.16931697726249695 Accuracy = 0.9625429511070251\n",
      "Epoch 2 Step 1800: Loss = 0.08319874852895737 Accuracy = 0.9634230732917786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 05:20:51.476178: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 3\n",
      "Epoch 3 Step 0: Loss = 0.04691037908196449 Accuracy = 1.0\n",
      "Epoch 3 Step 200: Loss = 0.07840126007795334 Accuracy = 0.9768345952033997\n",
      "Epoch 3 Step 400: Loss = 0.08920162916183472 Accuracy = 0.9734258055686951\n",
      "Epoch 3 Step 600: Loss = 0.045337602496147156 Accuracy = 0.973949670791626\n",
      "Epoch 3 Step 800: Loss = 0.04034687951207161 Accuracy = 0.9742509126663208\n",
      "Epoch 3 Step 1000: Loss = 0.16133856773376465 Accuracy = 0.9746503233909607\n",
      "Epoch 3 Step 1200: Loss = 0.08420872688293457 Accuracy = 0.9752029776573181\n",
      "Epoch 3 Step 1400: Loss = 0.07408581674098969 Accuracy = 0.9757985472679138\n",
      "Epoch 3 Step 1600: Loss = 0.12541890144348145 Accuracy = 0.9755621552467346\n",
      "Epoch 3 Step 1800: Loss = 0.06949835270643234 Accuracy = 0.9762111306190491\n",
      "Start of epoch 4\n",
      "Epoch 4 Step 0: Loss = 0.02844970114529133 Accuracy = 1.0\n",
      "Epoch 4 Step 200: Loss = 0.049619752913713455 Accuracy = 0.983208954334259\n",
      "Epoch 4 Step 400: Loss = 0.06432197988033295 Accuracy = 0.9817643165588379\n",
      "Epoch 4 Step 600: Loss = 0.05391557514667511 Accuracy = 0.9820091724395752\n",
      "Epoch 4 Step 800: Loss = 0.022992828860878944 Accuracy = 0.9821707010269165\n",
      "Epoch 4 Step 1000: Loss = 0.1170773133635521 Accuracy = 0.9824862480163574\n",
      "Epoch 4 Step 1200: Loss = 0.05275360494852066 Accuracy = 0.9824625253677368\n",
      "Epoch 4 Step 1400: Loss = 0.03562308847904205 Accuracy = 0.9826463460922241\n",
      "Epoch 4 Step 1600: Loss = 0.08309288322925568 Accuracy = 0.9826084971427917\n",
      "Epoch 4 Step 1800: Loss = 0.038467444479465485 Accuracy = 0.9829261302947998\n",
      "Start of epoch 5\n",
      "Epoch 5 Step 0: Loss = 0.01818396896123886 Accuracy = 1.0\n",
      "Epoch 5 Step 200: Loss = 0.03247785195708275 Accuracy = 0.9881840944290161\n",
      "Epoch 5 Step 400: Loss = 0.04944859817624092 Accuracy = 0.9876090884208679\n",
      "Epoch 5 Step 600: Loss = 0.03316640481352806 Accuracy = 0.9878847599029541\n",
      "Epoch 5 Step 800: Loss = 0.011790234595537186 Accuracy = 0.9877886772155762\n",
      "Epoch 5 Step 1000: Loss = 0.08558180928230286 Accuracy = 0.9878870844841003\n",
      "Epoch 5 Step 1200: Loss = 0.05385801941156387 Accuracy = 0.9875364303588867\n",
      "Epoch 5 Step 1400: Loss = 0.022861864417791367 Accuracy = 0.9875535368919373\n",
      "Epoch 5 Step 1600: Loss = 0.06903835386037827 Accuracy = 0.9874687790870667\n",
      "Epoch 5 Step 1800: Loss = 0.021091684699058533 Accuracy = 0.9877845644950867\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement the Custom Training Loop with Accuracy\n",
    "\n",
    "epochs = 5  # Number of epochs for training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Callback for Advanced Logging: \n",
    "\n",
    "Implement a custom callback to log additional metrics and information during training. \n",
    "\n",
    "#### 1. Set Up the Environment: \n",
    "\n",
    "Follow the setup from Exercise 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the Model: \n",
    "\n",
    "Use the same model as in Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define Loss Function, Optimizer, and Metric: \n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function and Adam optimizer. \n",
    "\n",
    "- Add Sparse Categorical Accuracy as a metric. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement the custom training loop with custom callback: \n",
    "\n",
    "Create a custom callback to log additional metrics at the end of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Step 4: Implement the Custom Callback \n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.3437376022338867 Accuracy = 0.125\n",
      "Epoch 1 Step 200: Loss = 0.360352098941803 Accuracy = 0.8387748599052429\n",
      "Epoch 1 Step 400: Loss = 0.20182159543037415 Accuracy = 0.8715710639953613\n",
      "Epoch 1 Step 600: Loss = 0.13571768999099731 Accuracy = 0.886283278465271\n",
      "Epoch 1 Step 800: Loss = 0.12405019253492355 Accuracy = 0.8986033201217651\n",
      "Epoch 1 Step 1000: Loss = 0.4779762625694275 Accuracy = 0.9060002565383911\n",
      "Epoch 1 Step 1200: Loss = 0.1485428810119629 Accuracy = 0.9121044874191284\n",
      "Epoch 1 Step 1400: Loss = 0.21721608936786652 Accuracy = 0.9169120192527771\n",
      "Epoch 1 Step 1600: Loss = 0.2547726333141327 Accuracy = 0.9199133515357971\n",
      "Epoch 1 Step 1800: Loss = 0.18704268336296082 Accuracy = 0.9239658713340759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 05:26:09.917325: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1, loss: 0.0354560986161232, accuracy: 0.9258666634559631\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.0923265814781189 Accuracy = 0.96875\n",
      "Epoch 2 Step 200: Loss = 0.14434149861335754 Accuracy = 0.9612873196601868\n",
      "Epoch 2 Step 400: Loss = 0.15293583273887634 Accuracy = 0.9589307904243469\n",
      "Epoch 2 Step 600: Loss = 0.0542994886636734 Accuracy = 0.9608985185623169\n",
      "Epoch 2 Step 800: Loss = 0.06734690070152283 Accuracy = 0.9620396494865417\n",
      "Epoch 2 Step 1000: Loss = 0.3818603754043579 Accuracy = 0.9625374674797058\n",
      "Epoch 2 Step 1200: Loss = 0.11561575531959534 Accuracy = 0.9638321995735168\n",
      "Epoch 2 Step 1400: Loss = 0.1388552188873291 Accuracy = 0.9646235108375549\n",
      "Epoch 2 Step 1600: Loss = 0.24689312279224396 Accuracy = 0.9642606377601624\n",
      "Epoch 2 Step 1800: Loss = 0.1260416954755783 Accuracy = 0.9651408791542053\n",
      "End of epoch 2, loss: 0.019565509632229805, accuracy: 0.9657833576202393\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
    "\n",
    "epochs = 2\n",
    "custom_callback = CustomCallback()  # Initialize the custom callback\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Call the custom callback at the end of each epoch\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()  # Use reset_state() instead of reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Add Hidden Layers \n",
    "\n",
    "Next, you will add a couple of hidden layers to your model. Hidden layers help the model learn complex patterns in the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(28, 28))  # Input layer with shape (28, 28)\n",
    "\n",
    "# Define hidden layers\n",
    "hidden_layer1 = Dense(64, activation='relu')(input_layer)  # First hidden layer with 64 neurons and ReLU activation\n",
    "hidden_layer2 = Dense(64, activation='relu')(hidden_layer1)  # Second hidden layer with 64 neurons and ReLU activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`Dense(64, activation='relu')` creates a dense (fully connected) layer with 64 units and ReLU activation function. \n",
    "\n",
    "Each hidden layer takes the output of the previous layer as its input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Define the output layer \n",
    "\n",
    "Finally, you will define the output layer. Suppose you are working on a binary classification problem, so the output layer will have one unit with a sigmoid activation function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`Dense(1, activation='sigmoid')` creates a dense layer with 1 unit and a sigmoid activation function, suitable for binary classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Create the Model \n",
    "\n",
    "Now, you will create the model by specifying the input and output layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`Model(inputs=input_layer, outputs=output_layer)` creates a Keras model that connects the input layer to the output layer through the hidden layers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Compile the Model \n",
    "\n",
    "Before training the model, you need to compile it. You will specify the loss function, optimizer, and evaluation metrics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`optimizer='adam'` specifies the Adam optimizer, a popular choice for training neural networks. \n",
    "\n",
    "`loss='binary_crossentropy'` specifies the loss function for binary classification problems. \n",
    "\n",
    "`metrics=['accuracy']` tells Keras to evaluate the model using accuracy during training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Train the Model \n",
    "\n",
    "You can now train the model on some training data. For this example, let's assume `X_train` is our training input data and `y_train` is the corresponding labels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5061 - loss: 0.6974   \n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5614 - loss: 0.6851 \n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4901 - loss: 0.6993 \n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5562 - loss: 0.6864 \n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5537 - loss: 0.6846 \n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5646 - loss: 0.6809 \n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5579 - loss: 0.6766 \n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5668 - loss: 0.6758 \n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5699 - loss: 0.6807 \n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5986 - loss: 0.6692 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f0a607d3c20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Redefine the Model for 20 features\n",
    "model = Sequential([\n",
    "    Input(shape=(20,)),  # Adjust input shape to (20,)\n",
    "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification with sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 2: Generate Example Data\n",
    "X_train = np.random.rand(1000, 20)  # 1000 samples, 20 features each\n",
    "y_train = np.random.randint(2, size=(1000, 1))  # 1000 binary labels (0 or 1)\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`X_train` and `y_train` are placeholders for your actual training data. \n",
    "\n",
    "`model.fit` trains the model for a specified number of epochs and batch size. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: Evaluate the Model \n",
    "\n",
    "After training, you can evaluate the model on test data to see how well it performs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5497 - loss: 0.6882  \n",
      "Test loss: 0.6845959424972534\n",
      "Test accuracy: 0.5550000071525574\n"
     ]
    }
   ],
   "source": [
    "# Example test data (in practice, use real dataset)\n",
    "X_test = np.random.rand(200, 20)  # 200 samples, 20 features each\n",
    "y_test = np.random.randint(2, size=(200, 1))  # 200 binary labels (0 or 1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`model.evaluate` computes the loss and accuracy of the model on test data. \n",
    "\n",
    "`X_test` and `y_test` are placeholders for your actual test data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises \n",
    "\n",
    "### Exercise 1: Basic Custom Training Loop \n",
    "\n",
    "#### Objective: Implement a basic custom training loop to train a simple neural network on the MNIST dataset. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "- Set up the environment and load the dataset. \n",
    "\n",
    "- Define the model with a Flatten layer and two Dense layers. \n",
    "\n",
    "- Define the loss function and optimizer. \n",
    "\n",
    "- Implement a custom training loop to iterate over the dataset, compute the loss, and update the model's weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.4479198455810547\n",
      "Epoch 1 Step 200: Loss = 0.35961779952049255\n",
      "Epoch 1 Step 400: Loss = 0.17092621326446533\n",
      "Epoch 1 Step 600: Loss = 0.142831489443779\n",
      "Epoch 1 Step 800: Loss = 0.17443600296974182\n",
      "Epoch 1 Step 1000: Loss = 0.44682055711746216\n",
      "Epoch 1 Step 1200: Loss = 0.18364255130290985\n",
      "Epoch 1 Step 1400: Loss = 0.2901107370853424\n",
      "Epoch 1 Step 1600: Loss = 0.22160889208316803\n",
      "Epoch 1 Step 1800: Loss = 0.15738002955913544\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.08581602573394775\n",
      "Epoch 2 Step 200: Loss = 0.11186428368091583\n",
      "Epoch 2 Step 400: Loss = 0.08300051838159561\n",
      "Epoch 2 Step 600: Loss = 0.0496271476149559\n",
      "Epoch 2 Step 800: Loss = 0.08186224102973938\n",
      "Epoch 2 Step 1000: Loss = 0.2409016489982605\n",
      "Epoch 2 Step 1200: Loss = 0.11728966236114502\n",
      "Epoch 2 Step 1400: Loss = 0.17968598008155823\n",
      "Epoch 2 Step 1600: Loss = 0.20095983147621155\n",
      "Epoch 2 Step 1800: Loss = 0.08832570165395737\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])\n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Step 4: Implement the Custom Training Loop\n",
    "epochs = 2\n",
    "# train_dataset = train_dataset.repeat(epochs)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)  # Forward pass\n",
    "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Logging the loss every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "# Step 2: Define the Model\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "\n",
    "# Step 4: Implement the Custom Training Loop\n",
    "for epoch in range(5): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Adding Accuracy Metric \n",
    "\n",
    "#### Objective: Enhance the custom training loop by adding an accuracy metric to monitor model performance. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "1. Set up the environment and define the model, loss function, and optimizer. \n",
    "\n",
    "2. Add Sparse Categorical Accuracy as a metric. \n",
    "\n",
    "3. Implement the custom training loop with accuracy tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.342799186706543 Accuracy = 0.21875\n",
      "Epoch 1 Step 200: Loss = 0.35739168524742126 Accuracy = 0.8339552283287048\n",
      "Epoch 1 Step 400: Loss = 0.18887975811958313 Accuracy = 0.8670511245727539\n",
      "Epoch 1 Step 600: Loss = 0.19254469871520996 Accuracy = 0.8821235299110413\n",
      "Epoch 1 Step 800: Loss = 0.17204798758029938 Accuracy = 0.894311785697937\n",
      "Epoch 1 Step 1000: Loss = 0.47377753257751465 Accuracy = 0.901098906993866\n",
      "Epoch 1 Step 1200: Loss = 0.1756671965122223 Accuracy = 0.9074208736419678\n",
      "Epoch 1 Step 1400: Loss = 0.2621147036552429 Accuracy = 0.9122501611709595\n",
      "Epoch 1 Step 1600: Loss = 0.2691260576248169 Accuracy = 0.9154239296913147\n",
      "Epoch 1 Step 1800: Loss = 0.1557587832212448 Accuracy = 0.9195238947868347\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.057567015290260315 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.1659086048603058 Accuracy = 0.9583333134651184\n",
      "Epoch 2 Step 400: Loss = 0.09465870261192322 Accuracy = 0.9562032222747803\n",
      "Epoch 2 Step 600: Loss = 0.07157367467880249 Accuracy = 0.958350658416748\n",
      "Epoch 2 Step 800: Loss = 0.09632965922355652 Accuracy = 0.9598158597946167\n",
      "Epoch 2 Step 1000: Loss = 0.26061931252479553 Accuracy = 0.960289716720581\n",
      "Epoch 2 Step 1200: Loss = 0.12247656285762787 Accuracy = 0.9613603353500366\n",
      "Epoch 2 Step 1400: Loss = 0.18088680505752563 Accuracy = 0.9621029496192932\n",
      "Epoch 2 Step 1600: Loss = 0.19471696019172668 Accuracy = 0.9618597626686096\n",
      "Epoch 2 Step 1800: Loss = 0.09365657716989517 Accuracy = 0.9627463817596436\n",
      "Start of epoch 3\n",
      "Epoch 3 Step 0: Loss = 0.02216915786266327 Accuracy = 1.0\n",
      "Epoch 3 Step 200: Loss = 0.10316884517669678 Accuracy = 0.9757462739944458\n",
      "Epoch 3 Step 400: Loss = 0.06959640234708786 Accuracy = 0.9731140732765198\n",
      "Epoch 3 Step 600: Loss = 0.02547468990087509 Accuracy = 0.9738976955413818\n",
      "Epoch 3 Step 800: Loss = 0.04853460565209389 Accuracy = 0.9740168452262878\n",
      "Epoch 3 Step 1000: Loss = 0.15686383843421936 Accuracy = 0.9744942784309387\n",
      "Epoch 3 Step 1200: Loss = 0.08176856487989426 Accuracy = 0.9749687910079956\n",
      "Epoch 3 Step 1400: Loss = 0.10387983918190002 Accuracy = 0.9751740097999573\n",
      "Epoch 3 Step 1600: Loss = 0.10310431569814682 Accuracy = 0.9750156402587891\n",
      "Epoch 3 Step 1800: Loss = 0.06589756160974503 Accuracy = 0.9754649996757507\n",
      "Start of epoch 4\n",
      "Epoch 4 Step 0: Loss = 0.012360643595457077 Accuracy = 1.0\n",
      "Epoch 4 Step 200: Loss = 0.04872121661901474 Accuracy = 0.9830535054206848\n",
      "Epoch 4 Step 400: Loss = 0.06401360034942627 Accuracy = 0.9819981455802917\n",
      "Epoch 4 Step 600: Loss = 0.021697981283068657 Accuracy = 0.9825291037559509\n",
      "Epoch 4 Step 800: Loss = 0.02517968788743019 Accuracy = 0.9824828505516052\n",
      "Epoch 4 Step 1000: Loss = 0.14535363018512726 Accuracy = 0.9824862480163574\n",
      "Epoch 4 Step 1200: Loss = 0.06092873960733414 Accuracy = 0.982384443283081\n",
      "Epoch 4 Step 1400: Loss = 0.04925299435853958 Accuracy = 0.9823786616325378\n",
      "Epoch 4 Step 1600: Loss = 0.057998333126306534 Accuracy = 0.9821791052818298\n",
      "Epoch 4 Step 1800: Loss = 0.0481000654399395 Accuracy = 0.9825097322463989\n",
      "Start of epoch 5\n",
      "Epoch 5 Step 0: Loss = 0.009625515900552273 Accuracy = 1.0\n",
      "Epoch 5 Step 200: Loss = 0.028745116665959358 Accuracy = 0.9861629605293274\n",
      "Epoch 5 Step 400: Loss = 0.0475664958357811 Accuracy = 0.9870635867118835\n",
      "Epoch 5 Step 600: Loss = 0.017346225678920746 Accuracy = 0.9873648285865784\n",
      "Epoch 5 Step 800: Loss = 0.023205723613500595 Accuracy = 0.9870474338531494\n",
      "Epoch 5 Step 1000: Loss = 0.10072817653417587 Accuracy = 0.987169086933136\n",
      "Epoch 5 Step 1200: Loss = 0.04298488050699234 Accuracy = 0.9870680570602417\n",
      "Epoch 5 Step 1400: Loss = 0.021338224411010742 Accuracy = 0.986817479133606\n",
      "Epoch 5 Step 1600: Loss = 0.04415452852845192 Accuracy = 0.9868636727333069\n",
      "Epoch 5 Step 1800: Loss = 0.02836386300623417 Accuracy = 0.9871252179145813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 05:37:52.953808: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])\n",
    "\n",
    "# Step 3- add acc\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n",
    "\n",
    "# Step 4: Implement the Custom Training Loop with Accuracy\n",
    "epochs = 5  # Number of epochs for training\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary><br>\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data() \n",
    "x_train = x_train / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "# Step 2: Define the Model\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "# Step 4: Implement the Custom Training Loop with Accuracy Tracking\n",
    "epochs = 5 \n",
    "for epoch in range(epochs): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.numpy()} Accuracy = {accuracy_metric.result().numpy()}') \n",
    "    accuracy_metric.reset_state() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Callback for Advanced Logging \n",
    "\n",
    "#### Objective: Implement a custom callback to log additional metrics and information during training. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "1. Set up the environment and define the model, loss function, optimizer, and metric. \n",
    "\n",
    "2. Create a custom callback to log additional metrics at the end of each epoch. \n",
    "\n",
    "3. Implement the custom training loop with the custom callback. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 0.006726743653416634 Accuracy = 1.0\n",
      "Epoch 1 Step 200: Loss = 0.012526403181254864 Accuracy = 0.9898942708969116\n",
      "Epoch 1 Step 400: Loss = 0.05455610528588295 Accuracy = 0.9899470210075378\n",
      "Epoch 1 Step 600: Loss = 0.01516730710864067 Accuracy = 0.9903806447982788\n",
      "Epoch 1 Step 800: Loss = 0.0207876805216074 Accuracy = 0.9901295304298401\n",
      "Epoch 1 Step 1000: Loss = 0.06033184751868248 Accuracy = 0.9901973009109497\n",
      "Epoch 1 Step 1200: Loss = 0.03475353494286537 Accuracy = 0.9901644587516785\n",
      "Epoch 1 Step 1400: Loss = 0.008299652487039566 Accuracy = 0.9900294542312622\n",
      "Epoch 1 Step 1600: Loss = 0.019367115572094917 Accuracy = 0.9899086356163025\n",
      "Epoch 1 Step 1800: Loss = 0.021505365148186684 Accuracy = 0.9900228977203369\n",
      "End of epoch 1, loss: 0.008011482656002045, accuracy: 0.9901833534240723\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.0042572240345180035 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.005575574468821287 Accuracy = 0.9912935495376587\n",
      "Epoch 2 Step 400: Loss = 0.03294113278388977 Accuracy = 0.9923628568649292\n",
      "Epoch 2 Step 600: Loss = 0.010004359297454357 Accuracy = 0.9931884407997131\n",
      "Epoch 2 Step 800: Loss = 0.010489728301763535 Accuracy = 0.9932116270065308\n",
      "Epoch 2 Step 1000: Loss = 0.07308418303728104 Accuracy = 0.9935064911842346\n",
      "Epoch 2 Step 1200: Loss = 0.027268562465906143 Accuracy = 0.9933128356933594\n",
      "Epoch 2 Step 1400: Loss = 0.006068997550755739 Accuracy = 0.9932191371917725\n",
      "Epoch 2 Step 1600: Loss = 0.014668557792901993 Accuracy = 0.9931878447532654\n",
      "Epoch 2 Step 1800: Loss = 0.014313749969005585 Accuracy = 0.9932850003242493\n",
      "End of epoch 2, loss: 0.031144961714744568, accuracy: 0.9933666586875916\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "\n",
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "# Step 4: Implement the Custom Callback \n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')\n",
    "\n",
    "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
    "epochs = 2\n",
    "custom_callback = CustomCallback()  # Initialize the custom callback\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Call the custom callback at the end of each epoch\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()  # Use reset_state() instead of reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "from tensorflow.keras.callbacks import Callback \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train = x_train / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "# Step 2: Define the Model\n",
    "model = Sequential([ \n",
    "    tf.keras.Input(shape=(28, 28)),  # Updated Input layer syntax\n",
    "    Flatten(), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "# Step 4: Implement the Custom Callback\n",
    "class CustomCallback(Callback): \n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}') \n",
    "\n",
    "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
    "custom_callback = CustomCallback() \n",
    "\n",
    "for epoch in range(5): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss.numpy(), 'accuracy': accuracy_metric.result().numpy()}) \n",
    "    accuracy_metric.reset_state()  # Updated method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Lab - Hyperparameter Tuning \n",
    "\n",
    "#### Enhancement: Add functionality to save the results of each hyperparameter tuning iteration as JSON files in a specified directory. \n",
    "\n",
    "#### Additional Instructions:\n",
    "\n",
    "Modify the tuning loop to save each iteration's results as JSON files.\n",
    "\n",
    "Specify the directory where these JSON files will be stored for easier retrieval and analysis of tuning results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 03s]\n",
      "val_accuracy: 0.9049999713897705\n",
      "\n",
      "Best val_accuracy So Far: 0.9350000023841858\n",
      "Total elapsed time: 00h 00m 34s\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "!pip install keras-tuner\n",
    "!pip install scikit-learn\n",
    "\n",
    "import json\n",
    "import os\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Step 2: Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                    activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification example\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Step 3: Initialize a Keras Tuner RandomSearch tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Set the number of trials\n",
    "    executions_per_trial=1,  # Set how many executions per trial\n",
    "    directory='tuning_results',  # Directory for saving logs\n",
    "    project_name='hyperparam_tuning'\n",
    ")\n",
    "\n",
    "# Step 4: Run the tuner search (make sure the data is correct)\n",
    "tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n",
    "\n",
    "# Step 5: Save the tuning results as JSON files\n",
    "try:\n",
    "    for i in range(10):\n",
    "        # Fetch the best hyperparameters from the tuner\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        \n",
    "        # Results dictionary to save hyperparameters and score\n",
    "        results = {\n",
    "            \"trial\": i + 1,\n",
    "            \"hyperparameters\": best_hps.values,  # Hyperparameters tuned in this trial\n",
    "            \"score\": None  # Add any score or metrics if available\n",
    "        }\n",
    "\n",
    "        # Save the results as JSON\n",
    "        with open(os.path.join('tuning_results', f\"trial_{i + 1}.json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "except IndexError:\n",
    "    print(\"Tuning process has not completed or no results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "!pip install keras-tuner\n",
    "!pip install scikit-learn\n",
    "\n",
    "import json\n",
    "import os\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Step 2: Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                    activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification example\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Step 3: Initialize a Keras Tuner RandomSearch tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Set the number of trials\n",
    "    executions_per_trial=1,  # Set how many executions per trial\n",
    "    directory='tuner_results',  # Directory for saving logs\n",
    "    project_name='hyperparam_tuning'\n",
    ")\n",
    "\n",
    "# Step 4: Run the tuner search (make sure the data is correct)\n",
    "tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n",
    "\n",
    "# Step 5: Save the tuning results as JSON files\n",
    "try:\n",
    "    for i in range(10):\n",
    "        # Fetch the best hyperparameters from the tuner\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        \n",
    "        # Results dictionary to save hyperparameters and score\n",
    "        results = {\n",
    "            \"trial\": i + 1,\n",
    "            \"hyperparameters\": best_hps.values,  # Hyperparameters tuned in this trial\n",
    "            \"score\": None  # Add any score or metrics if available\n",
    "        }\n",
    "\n",
    "        # Save the results as JSON\n",
    "        with open(os.path.join('tuning_results', f\"trial_{i + 1}.json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "except IndexError:\n",
    "    print(\"Tuning process has not completed or no results available.\")\n",
    " ```   \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Explanation of Hyperparameter Tuning\n",
    "\n",
    "**Addition to Explanation:** Add a note explaining the purpose of num_trials in the hyperparameter tuning context:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# It will return the best n sets of hyperparam sets to return. For example, if we chose 3, it return the top 3 sets of hyperparams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "Explanation: \"num_trials specifies the number of top hyperparameter sets to return. Setting num_trials=1 means that it will return only the best set of hyperparameters found during the tuning process.\"\n",
    " ```   \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "\n",
    "Congratulations on completing this lab! You have now successfully created, trained, and evaluated a simple neural network model using the Keras Functional API. This foundational knowledge will allow you to build more complex models and explore advanced functionalities in Keras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "48a1eb2565c8b635156cd21708473ccadb84e292e93f3530a9d5223b7590344e"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
